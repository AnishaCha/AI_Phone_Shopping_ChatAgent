"""
AI Phone Shopping Agent ‚Äî with Llama (Ollama) + JSON dataset
- Loads phones from phones.json
- Uses Llama (Ollama) for natural-language answers (fallback if unavailable)
- Supports search, compare, explain, follow-up, and details about a single phone
"""

import os
import re
import json
import streamlit as st
from typing import List, Dict, Any, Optional

# Try to import LangChain Ollama integration
LLM_AVAILABLE = True
try:
    from langchain_community.chat_models import ChatOllama
    from langchain.prompts import ChatPromptTemplate
except Exception:
    LLM_AVAILABLE = False

# ---------------- DB Loader (from JSON file) ---------------- #
@st.cache_data
def load_db_from_json() -> List[Dict[str, Any]]:
    with open("phones.json", "r", encoding="utf-8") as f:
        return json.load(f)

# ---------------- LLM helpers ---------------- #
def get_llm():
    if not LLM_AVAILABLE:
        return None
    try:
        return ChatOllama(model=os.getenv("OLLAMA_MODEL", "llama3.1:8b"), temperature=0.2)
    except Exception:
        return None

def llama_or_fallback(system_prompt: str, user_content: str, fallback: str) -> str:
    llm = get_llm()
    if not llm:
        print("‚ö†Ô∏è Llama not available ‚Üí using fallback heuristics.")
        return fallback
    try:
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("user", "{input}")
        ])
        msg = prompt.format_messages(input=user_content)
        resp = llm.invoke(msg)
        print("‚úÖ Response generated by Llama (Ollama).")
        return resp.content.strip()
    except Exception as e:
        print(f"‚ö†Ô∏è Error with Llama ‚Üí using fallback. Reason: {e}")
        return fallback

# ---------------- Parsing helpers ---------------- #
def _parse_budget_from_query(query: str) -> Optional[int]:
    if not query:
        return None
    m = re.search(r"(?:‚Çπ|rs\.?\s*)?\s*([0-9]{1,3}(?:,[0-9]{3})*|[0-9]+)\s*([kK])?", query)
    if not m:
        m = re.search(r"\b([0-9]{2,6})\s*(k|K)?\b", query)
    if m:
        num = m.group(1).replace(",", "")
        k = m.group(2)
        try:
            val = int(num)
            if k:
                return val * 1000 if val < 1000 else val
            return val
        except:
            return None
    return None

def _parse_price_relation(query: str) -> Optional[str]:
    q = (query or "").lower()
    if re.search(r"\b(under|below|less than|<=|<|upto|up to)\b", q):
        return "lte"
    if re.search(r"\b(above|over|more than|>=|>|at least)\b", q):
        return "gte"
    if re.search(r"\b(around|about|approx|~)\b", q):
        return "approx"
    return None

def normalize(s: str) -> str:
    return re.sub(r"[^\w\s]", "", (s or "").lower()).strip()

# ---------------- Scoring helpers ---------------- #
def camera_score(p: Dict[str, Any]) -> int:
    base = p.get("camera_mp", 0) // 5
    if p.get("ois"):
        base += 20
    return min(100, base)

# ---------------- Handlers ---------------- #
def handle_search(query: str) -> str:
    db = load_db_from_json()
    budget = _parse_budget_from_query(query)
    op = _parse_price_relation(query)
    qlower = query.lower()

    candidates = db
    if budget is not None:
        if op == "gte":
            candidates = [p for p in candidates if p["price"] >= budget]
        elif op == "approx":
            candidates = [p for p in candidates if abs(p["price"] - budget) <= max(3000, int(0.1 * budget))]
        else:
            candidates = [p for p in candidates if p["price"] <= budget]

    # If no matches, suggest closest options above the range
    if not candidates and budget is not None:
        close_alternatives = sorted(db, key=lambda p: abs(p["price"] - budget))
        candidates = close_alternatives[:5]
        message_prefix = f"No phones found near ‚Çπ{budget:,}. Showing closest matches instead:\n"
    else:
        message_prefix = "Results:\n"

    if not candidates:
        return "I couldn‚Äôt find any phones matching your criteria, even with closest matches."

    # Prioritize based on use-case
    if "camera" in qlower:
        candidates = sorted(candidates, key=lambda p: (camera_score(p), -p["price"]), reverse=True)
    elif "battery" in qlower or "fast charging" in qlower:
        candidates = sorted(candidates, key=lambda p: (p.get("battery_mah", 0) + p.get("charging_w", 0)), reverse=True)
    else:
        candidates = sorted(candidates, key=lambda p: p.get("price", 999999))

    top = candidates[:5]
    st.session_state["last_recs"] = top

    phone_lines = [f"{p['brand']} {p['model']} ‚Äî ‚Çπ{p['price']:,}, {p['battery_mah']}mAh, {p['charging_w']}W charging"
                   for p in top]

    rationale = llama_or_fallback(
        "You are a shopping assistant. Use only provided phone specs. Be concise and clear.",
        f"User query: {query}\nPhones: {json.dumps(top)}\nExplain why these are good matches for the query and summarize trade-offs.",
        "These are the closest matches for your request based on battery life, fast charging, and budget proximity."
    )

    return message_prefix + "\n".join(phone_lines) + "\n\nExplanation:\n" + rationale

def handle_compare(query: str) -> str:
    db = load_db_from_json()
    terms = [t.strip() for t in re.split(r"\bvs\b|,|and", query.lower()) if t.strip()]
    matches = []
    for term in terms:
        for p in db:
            if p["model"].lower() in term or p["brand"].lower() in term:
                if p not in matches:
                    matches.append(p)
    if len(matches) < 2:
        return "I couldn't find enough phones to compare."

    rationale = llama_or_fallback(
        "You are an expert phone advisor. Only use provided specs. Compare and explain trade-offs.",
        f"Compare these phones: {json.dumps(matches)}",
        f"Comparison between {matches[0]['brand']} {matches[0]['model']} and {matches[1]['brand']} {matches[1]['model']}."
    )
    return rationale

def handle_explain(query: str) -> str:
    ql = query.lower().strip()

    # Normalize for punctuation
    ql_clean = re.sub(r"[^a-z0-9\s]", "", ql)

    # Special case: both OIS and EIS
    if "ois" in ql_clean and "eis" in ql_clean:
        return llama_or_fallback(
            "You are a factual assistant. Explain differences between OIS and EIS in smartphone cameras. "
            "Use short bullet points, accurate and simple.",
            "Explain OIS vs EIS in cameras.",
            "OIS (Optical Image Stabilization) uses hardware to move the lens or sensor to counteract shake ‚Äî "
            "great for photos and low light. "
            "EIS (Electronic Image Stabilization) uses software to adjust video frames to reduce motion blur ‚Äî "
            "better for video stabilization."
        )

    # Fallback: handle single concept
    if "ois" in ql_clean:
        return "OIS (Optical Image Stabilization) uses hardware to move the lens or sensor to counteract shake ‚Äî especially useful in photos and low light."
    if "eis" in ql_clean:
        return "EIS (Electronic Image Stabilization) uses software algorithms to stabilize video by cropping and adjusting frames."

    # Otherwise let Llama try
    return llama_or_fallback(
        "You are a factual assistant. Explain this smartphone-related term in simple language.",
        query,
        "Information not in dataset."
    )


def handle_details(query: str) -> str:
    """Give description about a single phone if user asks for details.
    Fallbacks to `selected_phone` or single-item `last_recs` when user uses pronouns.
    """
    db = load_db_from_json()
    q = (query or "").lower()

    # 1) If user explicitly names model or brand, prefer that
    for p in db:
        if p["model"].lower() in q or p["brand"].lower() in q:
            return llama_or_fallback(
                "You are a shopping assistant. Write a short descriptive paragraph about this phone, "
                "highlighting its strengths and weaknesses, using only provided specs.",
                json.dumps(p),
                f"{p['brand']} {p['model']}: {p.get('camera_mp','?')}MP camera, {p.get('battery_mah','?')}mAh battery, "
                f"‚Çπ{p.get('price', '?'):,}, powered by {p.get('processor','?')}."
            )

    # 2) Pronoun-based fallback: if user said "this phone", "that phone", "it", etc.
    pronoun_tokens = ["this phone", "that phone", "the phone", "this", "that", "it"]
    if any(tok in q for tok in pronoun_tokens):
        # Prefer an explicitly selected phone (if you implement selecting cards)
        sel = st.session_state.get("selected_phone")
        if sel and isinstance(sel, dict):
            p = sel
            return llama_or_fallback(
                "You are a shopping assistant. Write a short descriptive paragraph about this phone, "
                "highlighting its strengths and weaknesses, using only provided specs.",
                json.dumps(p),
                f"{p['brand']} {p['model']}: {p.get('camera_mp','?')}MP camera, {p.get('battery_mah','?')}mAh battery, "
                f"‚Çπ{p.get('price', '?'):,}, powered by {p.get('processor','?')}."
            )
        # Otherwise try last_recs (the last search results)
        last = st.session_state.get("last_recs", [])
        if isinstance(last, list) and len(last) == 1:
            p = last[0]
            return llama_or_fallback(
                "You are a shopping assistant. Write a short descriptive paragraph about this phone, "
                "highlighting its strengths and weaknesses, using only provided specs.",
                json.dumps(p),
                f"{p['brand']} {p['model']}: {p.get('camera_mp','?')}MP camera, {p.get('battery_mah','?')}mAh battery, "
                f"‚Çπ{p.get('price', '?'):,}, powered by {p.get('processor','?')}."
            )
        # If multiple last_recs exist, return a helpful hint
        if isinstance(last, list) and len(last) > 1:
            return "I have multiple phones from your last search. Which one do you mean? (say the model name)"
    # 3) No identification possible
    return "I couldn‚Äôt identify the phone you want details about."


def handle_followup(query: str) -> str:
    last = st.session_state.get("last_recs", [])
    if not last:
        return "Please search for phones first."
    rationale = llama_or_fallback(
        "You are a phone shopping assistant. Write 2-3 sentences describing these phones, focusing on why someone might like them.",
        f"User follow-up: {query}\nPhones: {json.dumps(last)}",
        "\n".join([f"{p['brand']} {p['model']}: A {p['camera_mp']}MP camera, {p['battery_mah']}mAh battery, priced at ‚Çπ{p['price']:,}." for p in last])
    )
    return rationale

def handle_smalltalk(query: str) -> Optional[str]:
    q = query.lower().strip()
    if q in ("hi", "hello", "hey"):
        return "Hello I am your assistant."
    if "thank" in q:
        return "You're welcome!"
    return None

# ---------------- Dispatcher ---------------- #
def respond_to_query(query: str) -> str:
    if any(x in query.lower() for x in ["api key", "system prompt", "password", "secret"]):
        return "Sorry, I can't help with that."
    stq = handle_smalltalk(query)
    if stq:
        return stq
    ql = query.lower()
    if ql.startswith("explain") or "difference" in ql:
        return handle_explain(query)
    if "compare" in ql or " vs " in ql:
        return handle_compare(query)
    if "detail" in ql or "about" in ql:
        return handle_details(query)
    if any(tok in ql for tok in ["tell me more", "more about", "i like this phone"]):
        return handle_followup(query)
    return handle_search(query)

# ---------------- Streamlit Chat UI ---------------- #
st.set_page_config(page_title="AI Phone Shopping Agent", page_icon="üì±", layout="wide")
st.title("üì± AI Phone Shopping Agent (JSON dataset)")

llm = get_llm()
if llm:
    st.success("‚úÖ Powered by Llama (Ollama is running)")
else:
    st.warning("‚ö†Ô∏è Using fallback heuristics (Ollama not running)")

# Initialize session states
if "history" not in st.session_state:
    st.session_state["history"] = []
if "last_recs" not in st.session_state:
    st.session_state["last_recs"] = []

# Display all previous messages in scrollable chat
chat_container = st.container()
with chat_container:
    for chat in st.session_state["history"]:
        with st.chat_message("user"):
            st.markdown(chat["user"])
        with st.chat_message("assistant"):
            st.markdown(chat["assistant"])

# Chat input for new message
query = st.chat_input("Ask about phones...")

if query:
    # Add user message
    with st.chat_message("user"):
        st.markdown(query)

    reply = respond_to_query(query)

    # Add assistant reply
    with st.chat_message("assistant"):
        st.markdown(reply)

    # Save to history so chat persists
    st.session_state["history"].append({"user": query, "assistant": reply})



